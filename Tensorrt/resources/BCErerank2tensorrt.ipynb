{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/RAG/myenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XLMRobertaForSequenceClassification(\n",
       "  (roberta): XLMRobertaModel(\n",
       "    (embeddings): XLMRobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): XLMRobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): XLMRobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import os \n",
    "os.environ['HF_ENDPOINT']='https://hf-mirror.com'\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "file_path ='/root/RAG/Rerank/model'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokenizer = AutoTokenizer.from_pretrained('maidalun1020/bce-reranker-base_v1',cache_dir=file_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'maidalun1020/bce-reranker-base_v1',\n",
    "    device_map=device,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs=[[\"你好，好久不见\",\"是啊，好久不见\"],[\"你好，你叫什么名字\",\"我叫RAG，你好，你叫什么名字？\"]]\n",
    "output_ids = tokenizer(pairs, padding='max_length', truncation=True, return_tensors='pt', max_length=512)\n",
    "input_ids, attention_mask = output_ids['input_ids'], output_ids['attention_mask']\n",
    "scores = model(input_ids.to(device), attention_mask.to(device), return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[     0,      6, 124084,  ...,      1,      1,      1],\n",
       "        [     0,      6, 124084,  ...,      1,      1,      1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.SequenceClassifierOutput"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4136],\n",
       "        [0.3779]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将pytorch模型转化为onnx引擎\n",
    "#1、定义输入张量的形状信息\n",
    "input_id_=torch.randint(2,1000,(1,512)).to('cuda')\n",
    "attention_mask_=torch.ones(1,512).long().to('cuda')\n",
    "#转化模型\n",
    "import torch\n",
    "torch.onnx.export(\n",
    "    model, #原模型\n",
    "    (input_id_.to(torch.int64),attention_mask_.to(torch.int64)), #输入张量，接受一个张量或者元组\n",
    "    \"./rerank.onnx\",\n",
    "    export_params=True, #是否保存模型的权重信息\n",
    "    opset_version=17, #17支持INormalizationLayer，防止溢出\n",
    "    do_constant_folding=True,  #是否执行常量折叠优化\n",
    "    input_names=['input_ids','attention_mask'], #输入的名字\n",
    "    output_names=['output'],\n",
    "    dynamic_axes={\n",
    "        'input_ids':{0:'batch_size',1:'sequence_length'},\n",
    "        'attention_mask':{0:'batch_size',1:'sequence_length'},\n",
    "        'output':{0:'batch_size'}\n",
    "    }          #可变长度，在NLP中批次和序列长度都是可变长度\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将 PyTorch 模型转化为 ONNX 引擎\n",
    "# 1、定义输入张量的形状信息\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# 创建输入张量\n",
    "input_id_ = torch.randint(2, 1000, (1, 512),dtype=torch.int64).to('cuda')  # 在 GPU 上创建 input_ids\n",
    "attention_mask_ = torch.ones((1, 512), dtype=torch.int64).to('cuda')  # 正确创建 attention_mask 并转到 GPU\n",
    "\n",
    "# 转化模型\n",
    "torch.onnx.export(\n",
    "    model,  # 原模型\n",
    "    (input_id_, attention_mask_),  # 输入张量，接受一个张量或者元组\n",
    "    \"tensorrt_engine/embedding.onnx\",\n",
    "    export_params=True,  # 是否保存模型的权重信息\n",
    "    opset_version=17,  # 17支持 INormalizationLayer，防止溢出\n",
    "    do_constant_folding=True,  # 是否执行常量折叠优化\n",
    "    input_names=['input_ids', 'attention_mask'],  # 输入的名字\n",
    "    output_names=['last_hidden_state', 'pooler_output'],  # 输出的名字\n",
    "    dynamic_axes={\n",
    "        'input_ids': {0: 'batch_size', 1: 'sequence_length'},\n",
    "        'attention_mask': {0: 'batch_size', 1: 'sequence_length'},\n",
    "        'last_hidden_state': {0: 'batch_size', 1: 'sequence_length'},\n",
    "        'pooler_output': {0: 'batch_size'},\n",
    "    }  # 可变长度，在 NLP 中批次和序列长度都是可变长度\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt as trt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#将onnx转化为tensorrt引擎\n",
    "import sys\n",
    "sys.path.append('/root/anaconda3/lib/python3.11/site-packages')\n",
    "import tensorrt as trt\n",
    "logger=trt.Logger(trt.Logger.WARNING)\n",
    "trt.init_libnvinfer_plugins(logger,namespace='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "builder=trt.Builder(logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config=builder.create_builder_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.set_flag(trt.BuilderFlag.FP16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile = builder.create_optimization_profile()\n",
    "profile.set_shape(\"input_ids\", (20, 512),(64, 512),(200, 512))  # 输入的最小、默认批量大小、最大批次\n",
    "profile.set_shape(\"attention_mask\",(20,512),(64,512),(200,512))\n",
    "config.add_optimization_profile(profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE,1<<33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "network=builder.create_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser=trt.OnnxParser(network,logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11/28/2024-09:38:55] [TRT] [W] ModelImporter.cpp:429: Make sure input input_ids has Int64 binding.\n",
      "[11/28/2024-09:38:55] [TRT] [W] ModelImporter.cpp:429: Make sure input attention_mask has Int64 binding.\n"
     ]
    }
   ],
   "source": [
    "success=parser.parse_from_file('rerank.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "serialized_engine=builder.build_serialized_network(network,config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rerank_100k_8G_engine','wb') as f:\n",
    "          f.write(serialized_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11/28/2024-09:43:47] [TRT] [I] The logger passed into createInferRuntime differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value.\n",
      "[11/28/2024-09:43:47] [TRT] [I] Loaded engine size: 534 MiB\n",
      "[11/28/2024-09:43:47] [TRT] [I] [MS] Running engine with multi stream info\n",
      "[11/28/2024-09:43:47] [TRT] [I] [MS] Number of aux streams is 1\n",
      "[11/28/2024-09:43:47] [TRT] [I] [MS] Number of total worker streams is 2\n",
      "[11/28/2024-09:43:47] [TRT] [I] [MS] The main stream provided by execute/enqueue calls is the first worker stream\n",
      "[11/28/2024-09:43:47] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +1152, now: CPU 0, GPU 3363 (MiB)\n",
      "[[-1.37184450e-08]\n",
      " [-1.48835388e-08]\n",
      " [-1.44760790e-08]\n",
      " [-1.44760790e-08]\n",
      " [-1.47671217e-08]\n",
      " [-1.47671217e-08]\n",
      " [-1.45342876e-08]\n",
      " [-1.45342876e-08]\n",
      " [-1.44178705e-08]\n",
      " [-1.44178705e-08]\n",
      " [ 3.67140198e-43]\n",
      " [ 0.00000000e+00]\n",
      " [ 3.68541496e-43]\n",
      " [ 0.00000000e+00]\n",
      " [ 3.69942795e-43]\n",
      " [ 0.00000000e+00]\n",
      " [ 3.71344093e-43]\n",
      " [ 0.00000000e+00]\n",
      " [ 3.72745392e-43]\n",
      " [ 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "#CUDA_VISIBLE_DEVICES=0\n",
    "#os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "#读取tensorrt引擎\n",
    "import sys\n",
    "sys.path.append('/root/anaconda3/lib/python3.11/site-packages')\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "import tensorrt as trt\n",
    "logger=trt.Logger(trt.Logger.INFO)\n",
    "runtime=trt.Runtime(logger)\n",
    "trt.init_libnvinfer_plugins(logger,'')\n",
    "with open('rerank_100k_8G_engine','rb') as f:\n",
    "    serialized_engine=f.read()\n",
    "    engine=runtime.deserialize_cuda_engine(serialized_engine)\n",
    "import numpy as np\n",
    "with engine.create_execution_context() as context:\n",
    "    context.set_input_shape('attention_mask', (20,512))\n",
    "    context.set_input_shape('input_ids', (20,512))\n",
    "    input_data=np.arange(20,512,dtype=np.int64)\n",
    "    attention_mask=np.ones((20,512),dtype=np.int64)\n",
    "    pooler_output=np.empty((20,1),dtype=np.float32)\n",
    "    d_input_ids=cuda.mem_alloc(input_data.nbytes)\n",
    "    d_input_mask=cuda.mem_alloc(input_data.nbytes)\n",
    "    d_pooler_output=cuda.mem_alloc(pooler_output.nbytes)\n",
    "    context.set_tensor_address('input_ids', int(d_input_ids))\n",
    "    context.set_tensor_address('attention_mask',int(d_input_mask))\n",
    "    context.set_tensor_address('output', int(d_pooler_output))\n",
    "    stream=cuda.Stream()\n",
    "    cuda.memcpy_htod_async(d_input_ids,input_data,stream)\n",
    "    cuda.memcpy_htod_async(d_input_mask,input_data,stream)\n",
    "    bindings = [int(d_input_ids),int(d_input_mask),int(d_pooler_output)]\n",
    "    context.execute_v2(bindings)\n",
    "    cuda.memcpy_dtoh(pooler_output,d_pooler_output)\n",
    "\n",
    "print(pooler_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
