{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n将重排序模型BCE转化为tensorrt engine\\n\\n将向量模型转化为tensorrt engine\\n\\n压缩模型可能需要自己实现论文算法与tensorrt引擎进行匹配\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#目前要做的工作\n",
    "\"\"\" \n",
    "将重排序模型BCE转化为tensorrt engine\n",
    "\n",
    "将向量模型转化为tensorrt engine\n",
    "\n",
    "压缩模型可能需要自己实现论文算法与tensorrt引擎进行匹配\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/autodl-tmp/3.0/Tensorrt/resources/../../Engine/model/.cache\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "file_dir=os.getcwd()\n",
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "file_dir=os.path.join(file_dir,'../')\n",
    "config.read('../cache.ini')\n",
    "file_name=config['settings']['HF_HOME']\n",
    "file_path=os.path.join(file_dir,file_name)\n",
    "print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/rag/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/miniconda3/envs/rag/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "2024-12-13 14:02:48.405943: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-13 14:02:48.410487: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-12-13 14:02:48.410501: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XLMRobertaModel(\n",
       "  (embeddings): XLMRobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): XLMRobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x XLMRobertaLayer(\n",
       "        (attention): XLMRobertaAttention(\n",
       "          (self): XLMRobertaSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): XLMRobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): XLMRobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): XLMRobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): XLMRobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION']='python'\n",
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "file_dir=os.getcwd()\n",
    "file_dir=os.path.join(file_dir,'../')\n",
    "config.read('../cache.ini')\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = os.path.join(file_dir,config['settings']['HF_DATASETS_CACHE'])\n",
    "os.environ[\"HF_HOME\"] =os.path.join(file_dir,config['settings']['HF_HOME'])\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = os.path.join(file_dir,config['settings']['HUGGINGFACE_HUB_CACHE'])\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = os.path.join(file_dir,config['settings']['TRANSFORMERS_CACHE'])\n",
    "os.environ[\"HF_ENDPOINT\"] =config['settings']['HF_ENDPOINT']\n",
    "os.environ[\"XDG_CACHE_HOME\"] = os.path.join(file_dir,config['settings']['XDG_CACHE_HOME'])\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "# list of sentences\n",
    "import torch\n",
    "torch.backends.cuda.enable_flash_sdp(enabled=True)\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(enabled=True)\n",
    "torch.backends.cuda.enable_math_sdp(enabled=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained('maidalun1020/bce-embedding-base_v1',cache_dir=os.path.join(file_dir,config['settings']['TRANSFORMERS_CACHE']))\n",
    "model = AutoModel.from_pretrained('maidalun1020/bce-embedding-base_v1',torch_dtype=torch.float32,device_map='auto',cache_dir=os.path.join(file_dir, config['settings']['TRANSFORMERS_CACHE']))\n",
    "model.eval()\n",
    "#device = 'cuda'  # if no GPU, set \"cpu\"\n",
    "#model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMRobertaModel(\n",
       "  (embeddings): XLMRobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): XLMRobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x XLMRobertaLayer(\n",
       "        (attention): XLMRobertaAttention(\n",
       "          (self): XLMRobertaSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): XLMRobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): XLMRobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): XLMRobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): XLMRobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: onnx in /root/miniconda3/envs/rag/lib/python3.10/site-packages (1.12.0)\n",
      "Requirement already satisfied: onnxruntime in /root/miniconda3/envs/rag/lib/python3.10/site-packages (1.12.1)\n",
      "Requirement already satisfied: pycuda in /root/miniconda3/envs/rag/lib/python3.10/site-packages (2024.1.2)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /root/miniconda3/envs/rag/lib/python3.10/site-packages (from onnx) (1.26.4)\n",
      "Collecting protobuf<=3.20.1,>=3.12.2 (from onnx)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/4c/be/bdd22d86d24e5b8b08673d80be70d1a72c255f85152ff09b28490904092a/protobuf-3.20.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.2.1 in /root/miniconda3/envs/rag/lib/python3.10/site-packages (from onnx) (4.12.2)\n",
      "Requirement already satisfied: coloredlogs in /root/miniconda3/envs/rag/lib/python3.10/site-packages (from onnxruntime) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /root/miniconda3/envs/rag/lib/python3.10/site-packages (from onnxruntime) (1.12)\n",
      "Requirement already satisfied: packaging in /root/miniconda3/envs/rag/lib/python3.10/site-packages (from onnxruntime) (24.1)\n",
      "Requirement already satisfied: sympy in /root/miniconda3/envs/rag/lib/python3.10/site-packages (from onnxruntime) (1.13.1)\n",
      "Requirement already satisfied: pytools>=2011.2 in /root/miniconda3/envs/rag/lib/python3.10/site-packages (from pycuda) (2024.1.19)\n",
      "Requirement already satisfied: platformdirs>=2.2.0 in /root/miniconda3/envs/rag/lib/python3.10/site-packages (from pycuda) (3.10.0)\n",
      "Requirement already satisfied: mako in /root/miniconda3/envs/rag/lib/python3.10/site-packages (from pycuda) (1.3.8)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /root/miniconda3/envs/rag/lib/python3.10/site-packages (from coloredlogs->onnxruntime) (10.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /root/miniconda3/envs/rag/lib/python3.10/site-packages (from mako->pycuda) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /root/miniconda3/envs/rag/lib/python3.10/site-packages (from sympy->onnxruntime) (1.3.0)\n",
      "Installing collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Successfully uninstalled protobuf-3.20.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorboard 2.9.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.1 which is incompatible.\n",
      "tensorflow-gpu 2.9.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed protobuf-3.20.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install onnx onnxruntime pycuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt as trt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cuda\"\n",
    "def embedding(text_list):\n",
    "    inputs = tokenizer(text_list, padding=True, truncation=True, max_length=520, return_tensors=\"pt\")\n",
    "    inputs_on_device = {k: v.to(device) for k, v in inputs.items()}\n",
    "    # get embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs_on_device, return_dict=True)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs=embedding([\"你好，好久不见\"])\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 768])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0, 35378,     4,   759, 10269,    83, 99942,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer([\"Hello, my dog is cute\"], padding=True, truncation=True, max_length=520, return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将 PyTorch 模型转化为 ONNX 引擎\n",
    "# 1、定义输入张量的形状信息\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# 创建输入张量\n",
    "input_id_ = torch.randint(2, 1000, (1, 512),dtype=torch.int64).to('cuda')  # 在 GPU 上创建 input_ids\n",
    "attention_mask_ = torch.ones((1, 512), dtype=torch.int64).to('cuda')  # 正确创建 attention_mask 并转到 GPU\n",
    "\n",
    "# 转化模型\n",
    "torch.onnx.export(\n",
    "    model,  # 原模型\n",
    "    (input_id_, attention_mask_),  # 输入张量，接受一个张量或者元组\n",
    "    \"engine/BCEembedding.onnx\",\n",
    "    export_params=True,  # 是否保存模型的权重信息\n",
    "    opset_version=17,  # 17支持 INormalizationLayer，防止溢出\n",
    "    do_constant_folding=True,  # 是否执行常量折叠优化\n",
    "    input_names=['input_ids', 'attention_mask'],  # 输入的名字\n",
    "    output_names=['last_hidden_state', 'pooler_output'],  # 输出的名字\n",
    "    dynamic_axes={\n",
    "        'input_ids': {0: 'batch_size', 1: 'sequence_length'},\n",
    "        'attention_mask': {0: 'batch_size', 1: 'sequence_length'},\n",
    "        'last_hidden_state': {0: 'batch_size', 1: 'sequence_length'},\n",
    "        'pooler_output': {0: 'batch_size'},\n",
    "    }  # 可变长度，在 NLP 中批次和序列长度都是可变长度\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorrt as trt\n",
    "logger=trt.Logger(trt.Logger.WARNING)\n",
    "trt.init_libnvinfer_plugins(logger,namespace='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder=trt.Builder(logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config=builder.create_builder_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.set_flag(trt.BuilderFlag.FP16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-11:03:03] [TRT] [W] The implicit batch dimension mode has been deprecated. Please create the network with NetworkDefinitionCreationFlag::kEXPLICIT_BATCH flag whenever possible.\n"
     ]
    }
   ],
   "source": [
    "network = builder.create_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "network=builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile = builder.create_optimization_profile()\n",
    "profile.set_shape(\"input_ids\", (1, 1),(4, 256),(8, 512))  # 输入的最小、默认批量大小、最大批次\n",
    "profile.set_shape(\"attention_mask\",(1,1),(4,256),(8,512))\n",
    "config.add_optimization_profile(profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE,1<<30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser=trt.OnnxParser(network,logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[libprotobuf WARNING google/protobuf/io/coded_stream.cc:604] Reading dangerously large protocol message.  If the message turns out to be larger than 2147483647 bytes, parsing will be halted for security reasons.  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.\n",
      "[libprotobuf WARNING google/protobuf/io/coded_stream.cc:81] The total number of bytes read was 1112412160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/13/2024-14:04:14] [TRT] [W] onnx2trt_utils.cpp:374: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "[12/13/2024-14:04:14] [TRT] [W] onnx2trt_utils.cpp:400: One or more weights outside the range of INT32 was clamped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[libprotobuf WARNING google/protobuf/io/coded_stream.cc:604] Reading dangerously large protocol message.  If the message turns out to be larger than 2147483647 bytes, parsing will be halted for security reasons.  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.\n",
      "[libprotobuf WARNING google/protobuf/io/coded_stream.cc:81] The total number of bytes read was 1112412160\n"
     ]
    }
   ],
   "source": [
    "success=parser.parse_from_file('engine/BCEembedding.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(success)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(parser.num_errors):\n",
    "    print(parser.get_error(idx))\n",
    "if not success:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/autodl-tmp/3.0/Tensorrt/resources/../../Engine/model/.cache\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/13/2024-14:05:04] [TRT] [W] TensorRT encountered issues when converting weights between types and that could affect accuracy.\n",
      "[12/13/2024-14:05:04] [TRT] [W] If this is not the desired behavior, please modify the weights or retrain with regularization to adjust the magnitude of the weights.\n",
      "[12/13/2024-14:05:04] [TRT] [W] Check verbose logs for the list of affected weights.\n",
      "[12/13/2024-14:05:04] [TRT] [W] - 118 weights are affected by this issue: Detected subnormal FP16 values.\n",
      "[12/13/2024-14:05:04] [TRT] [W] - 46 weights are affected by this issue: Detected values less than smallest positive FP16 subnormal value and converted them to the FP16 minimum subnormalized value.\n",
      "[12/13/2024-14:05:04] [TRT] [W] - 1 weights are affected by this issue: Detected finite FP32 values which would overflow in FP16 and converted them to the closest finite FP16 value.\n"
     ]
    }
   ],
   "source": [
    "serialized_engine=builder.build_serialized_network(network,config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('engine/BCEembedding_engine','wb') as f:\n",
    "          f.write(serialized_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/13/2024-14:05:52] [TRT] [I] The logger passed into createInferRuntime differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value.\n",
      "[12/13/2024-14:05:52] [TRT] [I] Loaded engine size: 532 MiB\n",
      "[12/13/2024-14:05:52] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +530, now: CPU 0, GPU 530 (MiB)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#读取tensorrt引擎\n",
    "import tensorrt as trt\n",
    "logger=trt.Logger(trt.Logger.INFO)\n",
    "runtime=trt.Runtime(logger)\n",
    "trt.init_libnvinfer_plugins(logger,'')\n",
    "with open('engine/BCEembedding_engine','rb') as f:\n",
    "    serialized_engine=f.read()\n",
    "    engine=runtime.deserialize_cuda_engine(serialized_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor Name:input_ids,Shape:(-1, -1),Size:1,Data Type:<class 'numpy.int32'>\n",
      "Tensor Name:attention_mask,Shape:(-1, -1),Size:1,Data Type:<class 'numpy.int32'>\n",
      "Tensor Name:last_hidden_state,Shape:(-1, -1, 768),Size:768,Data Type:<class 'numpy.float32'>\n",
      "Tensor Name:pooler_output,Shape:(-1, 768),Size:-768,Data Type:<class 'numpy.float32'>\n"
     ]
    }
   ],
   "source": [
    "for i in range(engine.num_io_tensors):\n",
    "    tensor_name=engine.get_tensor_name(i)\n",
    "    tensor_shape=engine.get_tensor_shape(tensor_name)\n",
    "    size=trt.volume(tensor_shape)\n",
    "    dtype=trt.nptype(engine.get_tensor_dtype(tensor_name))\n",
    "    \n",
    "    print(f\"Tensor Name:{tensor_name},Shape:{tensor_shape},Size:{size},Data Type:{dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "input_data=np.arange(1,4,dtype=np.int64)\n",
    "attention_mask=np.ones((1,4),dtype=np.int64)\n",
    "last_hidden_state=np.empty((1,4,768),dtype=np.float32)\n",
    "pooler_output=np.empty((1,768),dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"你好\", padding=True, truncation=True, max_length=520,return_tensors=\"pt\")\n",
    "inputs['input_ids'].device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/13/2024-14:05:59] [TRT] [I] The logger passed into createInferRuntime differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value.\n",
      "[12/13/2024-14:06:00] [TRT] [I] Loaded engine size: 532 MiB\n",
      "[12/13/2024-14:06:00] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +530, now: CPU 0, GPU 1060 (MiB)\n",
      "[12/13/2024-14:06:00] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +148, now: CPU 0, GPU 678 (MiB)\n",
      "[[-2.35595703e-02 -2.06909180e-02  3.41796875e-02 -6.09588623e-03\n",
      "   1.20727539e-01  5.86242676e-02 -1.54495239e-03  5.39245605e-02\n",
      "   9.83886719e-02  4.61730957e-02 -6.53457642e-03  9.34448242e-02\n",
      "   5.21545410e-02  1.25579834e-02  7.26318359e-03  4.48913574e-02\n",
      "  -3.54003906e-02  4.78744507e-03 -7.43408203e-02  1.62719727e-01\n",
      "  -6.06994629e-02  1.60827637e-02 -4.13894653e-03 -1.86767578e-02\n",
      "  -6.34155273e-02  8.36791992e-02 -1.99096680e-01 -3.06854248e-02\n",
      "   1.27807617e-01 -3.80249023e-02  1.14257812e-01  3.67736816e-02\n",
      "   2.28271484e-02  2.42614746e-02  4.76989746e-02 -3.53088379e-02\n",
      "  -5.47790527e-02  1.05381012e-03  4.08935547e-02  1.88476562e-01\n",
      "   7.62557983e-03  1.33789062e-01 -8.18481445e-02  1.98059082e-02\n",
      "   4.31518555e-02 -2.03735352e-01 -1.66870117e-01  9.08813477e-02\n",
      "   1.61621094e-01 -1.03302002e-02  3.28063965e-02  1.76544189e-02\n",
      "   5.68847656e-02 -3.10821533e-02  3.12805176e-02 -2.63977051e-02\n",
      "   3.86047363e-02  5.64880371e-02  7.91625977e-02  5.52368164e-02\n",
      "  -7.09838867e-02 -6.78100586e-02  9.31396484e-02 -1.22924805e-01\n",
      "   4.10156250e-02 -4.15344238e-02 -8.96453857e-03 -1.64794922e-01\n",
      "   2.09472656e-01 -1.83471680e-01 -1.21887207e-01 -1.31835938e-01\n",
      "  -1.84478760e-02  1.67480469e-01 -2.58636475e-02  2.25677490e-02\n",
      "  -1.06689453e-01  1.28784180e-01 -1.25579834e-02  2.86865234e-02\n",
      "   2.36053467e-02  1.08276367e-01  1.65710449e-02 -7.28149414e-02\n",
      "   8.37402344e-02 -1.17675781e-01 -8.33129883e-02 -5.01708984e-02\n",
      "  -6.03942871e-02 -1.70593262e-02  5.43212891e-02  4.18395996e-02\n",
      "   8.31909180e-02  2.86254883e-02 -6.78100586e-02 -9.28955078e-02\n",
      "   1.59545898e-01 -6.27441406e-02 -7.87963867e-02  8.32519531e-02\n",
      "  -1.09497070e-01 -1.00326538e-02  5.95703125e-02 -4.09240723e-02\n",
      "  -1.64916992e-01 -8.67919922e-02 -1.77612305e-01 -8.75854492e-02\n",
      "   1.26220703e-01 -8.76464844e-02 -1.42059326e-02  3.23181152e-02\n",
      "  -7.21435547e-02 -5.05065918e-03  2.28118896e-02  7.97119141e-02\n",
      "   1.17206573e-03  6.30493164e-02 -1.76269531e-01 -6.43310547e-02\n",
      "  -7.02285767e-03  2.08374023e-01 -1.46606445e-01 -2.02026367e-02\n",
      "  -3.39965820e-02 -5.80444336e-02 -2.78015137e-02 -1.20666504e-01\n",
      "  -8.52050781e-02 -9.37652588e-03 -4.38232422e-02  4.02832031e-03\n",
      "   1.11236572e-02 -2.84912109e-01 -6.86035156e-02 -8.36181641e-02\n",
      "   1.84326172e-01  1.26708984e-01 -1.65405273e-01 -4.77600098e-02\n",
      "   6.24847412e-03  1.33743286e-02  1.18484497e-02 -1.44424438e-02\n",
      "  -5.34973145e-02  2.68066406e-01  1.35345459e-02  1.69311523e-01\n",
      "  -4.06188965e-02  4.19616699e-02  4.65774536e-03 -1.27197266e-01\n",
      "  -5.79589844e-01  1.44165039e-01 -7.82470703e-02 -1.48773193e-03\n",
      "   7.94677734e-02  1.85394287e-02 -1.88751221e-02 -5.09033203e-02\n",
      "  -4.75463867e-02  1.25122070e-01  7.78198242e-02 -1.17065430e-01\n",
      "   1.39526367e-01  6.39648438e-02  2.40631104e-02  1.91894531e-01\n",
      "  -1.16760254e-01 -5.61523438e-02  9.24682617e-02 -1.60156250e-01\n",
      "   5.57861328e-02 -8.16345215e-03 -1.14593506e-02  1.98516846e-02\n",
      "   8.09936523e-02  6.38427734e-02 -1.06445312e-01 -2.53601074e-02\n",
      "   1.44042969e-01  3.47595215e-02 -9.49096680e-02  2.58544922e-01\n",
      "   5.76400757e-03 -1.56005859e-01  7.77587891e-02 -5.23376465e-02\n",
      "   1.50146484e-01 -1.77246094e-01 -1.32568359e-01  6.65283203e-02\n",
      "   1.57226562e-01  4.00085449e-02 -3.50952148e-02 -1.73211098e-04\n",
      "  -8.10546875e-02  1.00097656e-01 -3.10058594e-02 -1.12670898e-01\n",
      "  -3.13720703e-02 -8.41064453e-02  2.06909180e-02  1.68151855e-02\n",
      "  -4.84008789e-02 -9.38720703e-02 -1.60400391e-01  3.94592285e-02\n",
      "   1.14212036e-02  7.84301758e-02  3.42712402e-02 -1.08642578e-01\n",
      "   2.94494629e-02 -7.14111328e-02 -4.09240723e-02  3.50646973e-02\n",
      "  -4.18090820e-02 -3.89404297e-02  1.94580078e-01 -1.10534668e-01\n",
      "   1.25503540e-02 -3.80554199e-02  1.17721558e-02 -8.60595703e-02\n",
      "   1.27075195e-01  5.96923828e-02 -9.52148438e-02  2.47955322e-02\n",
      "   4.06188965e-02 -5.66711426e-02 -1.25366211e-01 -3.12194824e-02\n",
      "   2.72216797e-01 -9.72747803e-03 -4.04968262e-02 -1.38671875e-01\n",
      "  -1.02478027e-01 -7.11441040e-03 -2.41088867e-02  2.03247070e-01\n",
      "  -2.65960693e-02  5.91278076e-03  6.35986328e-02 -2.52532959e-02\n",
      "   1.35131836e-01 -5.69763184e-02 -1.10931396e-02  1.35879517e-02\n",
      "   3.05175781e-02 -1.74194336e-01  1.77124023e-01 -7.96508789e-02\n",
      "   5.81054688e-02  1.49688721e-02 -8.48999023e-02 -2.77404785e-02\n",
      "  -8.27636719e-02  7.95288086e-02  5.71594238e-02 -4.08691406e-01\n",
      "   2.32696533e-02  2.26806641e-01  6.45141602e-02  1.82983398e-01\n",
      "  -5.73425293e-02 -7.14721680e-02  7.90405273e-02  1.09069824e-01\n",
      "   4.28466797e-02  1.14379883e-01 -2.16918945e-01  4.95910645e-02\n",
      "   1.46865845e-02  2.04315186e-02  2.87818909e-03  1.13891602e-01\n",
      "  -2.13623047e-02 -1.15966797e-01  4.72412109e-02  3.41796875e-01\n",
      "   5.09643555e-02 -1.05712891e-01 -1.29760742e-01  1.66870117e-01\n",
      "   8.66699219e-02 -7.11822510e-03 -3.59535217e-03 -1.50878906e-01\n",
      "   1.80297852e-01  3.03497314e-02  5.02319336e-02 -4.83093262e-02\n",
      "   1.26953125e-02  8.27789307e-04 -3.13415527e-02 -2.42919922e-02\n",
      "  -5.59387207e-02 -7.28759766e-02 -5.10559082e-02 -9.53369141e-02\n",
      "   9.26513672e-02  3.08532715e-02 -8.03833008e-02 -1.67846680e-01\n",
      "   1.33819580e-02 -1.54907227e-01  1.94244385e-02  2.80517578e-01\n",
      "  -8.76464844e-02 -1.64672852e-01 -3.29284668e-02 -9.18579102e-03\n",
      "  -1.07177734e-01  7.95288086e-02  9.37500000e-02  3.57055664e-02\n",
      "   7.95288086e-02 -2.74810791e-02  9.74731445e-02 -1.05590820e-01\n",
      "   7.97729492e-02 -1.08703613e-01 -6.14929199e-02  2.27294922e-01\n",
      "   1.39770508e-02 -5.29098511e-03 -1.75323486e-02 -9.16748047e-02\n",
      "  -1.16157532e-03  3.28674316e-02 -2.26074219e-01 -3.67431641e-02\n",
      "   1.64916992e-01  9.08203125e-02  6.12792969e-02  1.23310089e-03\n",
      "   4.69207764e-03 -5.76171875e-02  1.66137695e-01  1.22833252e-03\n",
      "   4.61120605e-02  1.13220215e-01  1.14212036e-02  4.40063477e-02\n",
      "  -1.63116455e-02  6.81152344e-02  6.57958984e-02 -3.02276611e-02\n",
      "   8.08105469e-02  4.11376953e-02  5.77087402e-02  3.88488770e-02\n",
      "   1.64306641e-01  8.09326172e-02 -5.81665039e-02 -5.86242676e-02\n",
      "   2.62603760e-02 -1.13586426e-01  3.85131836e-02 -3.96118164e-02\n",
      "  -1.06201172e-01 -9.07592773e-02 -1.04431152e-01  7.80639648e-02\n",
      "  -1.42974854e-02  4.08325195e-02 -5.57250977e-02 -1.24450684e-01\n",
      "   1.45568848e-02  1.17492676e-01  4.25720215e-02  2.99835205e-02\n",
      "  -6.93969727e-02  6.85424805e-02  3.56140137e-02  9.69238281e-02\n",
      "  -1.64672852e-01  1.22192383e-01  6.25991821e-03 -1.08642578e-01\n",
      "   6.57653809e-03  3.17382812e-01  2.07519531e-02  8.08105469e-02\n",
      "   2.52929688e-01  7.22045898e-02 -3.27148438e-02 -2.69775391e-01\n",
      "   3.65600586e-02 -1.46240234e-01 -3.47900391e-02 -1.28021240e-02\n",
      "  -8.77380371e-03 -3.85131836e-02  1.21459961e-01 -5.00488281e-02\n",
      "   1.83105469e-01  2.61230469e-02 -3.54003906e-02 -4.13818359e-02\n",
      "  -1.58935547e-01 -8.11157227e-02 -1.64672852e-01  9.99755859e-02\n",
      "   3.12805176e-02 -1.56860352e-01 -2.45208740e-02 -4.97436523e-03\n",
      "  -9.22851562e-02  1.66015625e-01 -1.92993164e-01 -7.65609741e-03\n",
      "  -2.19360352e-01 -1.57470703e-01  1.41357422e-01  3.69262695e-02\n",
      "   1.23748779e-02 -1.12548828e-01 -6.03675842e-04 -9.55200195e-02\n",
      "  -2.72674561e-02  1.04125977e-01 -1.15966797e-01  6.38427734e-02\n",
      "   8.75244141e-02 -6.19506836e-02 -2.54211426e-02  3.71093750e-02\n",
      "  -5.56640625e-02 -2.38494873e-02  5.07812500e-02 -2.17132568e-02\n",
      "  -1.09069824e-01  7.42187500e-02  2.71606445e-02 -1.18347168e-01\n",
      "   1.86157227e-01  4.71191406e-02 -2.42614746e-03  9.03930664e-02\n",
      "  -1.87835693e-02  1.71508789e-01 -1.61743164e-01 -2.98309326e-02\n",
      "  -8.20922852e-02 -8.19091797e-02  1.12426758e-01 -1.33789062e-01\n",
      "  -1.08642578e-01  1.62506104e-02  1.38549805e-01 -8.88061523e-02\n",
      "   1.04125977e-01  2.38494873e-02 -1.08337402e-01  2.37579346e-02\n",
      "   3.45153809e-02  1.05056763e-02 -1.13677979e-03  1.03698730e-01\n",
      "   1.90429688e-01  9.91058350e-03  9.58862305e-02  6.68334961e-02\n",
      "   3.76281738e-02  9.56420898e-02 -2.56347656e-02  1.69525146e-02\n",
      "   1.07910156e-01 -1.91040039e-02  1.12380981e-02  6.08215332e-02\n",
      "   1.49902344e-01  2.19879150e-02 -8.25195312e-02  5.30090332e-02\n",
      "  -2.35290527e-02 -1.52954102e-01  6.65893555e-02  1.65100098e-02\n",
      "   1.69067383e-01  6.22863770e-02  5.50537109e-02 -1.28051758e-01\n",
      "   1.87377930e-02  1.33789062e-01 -3.51562500e-02  1.96533203e-02\n",
      "   1.78344727e-01 -7.06787109e-02 -3.31420898e-02 -8.89778137e-04\n",
      "   8.12377930e-02 -1.81884766e-01  1.32446289e-02 -4.57763672e-02\n",
      "   2.24914551e-02 -5.56030273e-02 -3.68652344e-02  5.65185547e-02\n",
      "  -9.02099609e-02 -2.12097168e-02  3.17077637e-02 -5.02929688e-02\n",
      "  -1.23657227e-01 -4.85839844e-02 -3.14941406e-02 -3.07617188e-02\n",
      "  -4.86450195e-02 -1.21704102e-01  1.13891602e-01 -6.35986328e-02\n",
      "  -7.72705078e-02  3.63159180e-02  1.31835938e-01  1.28906250e-01\n",
      "  -4.22363281e-02  1.07879639e-02  1.46255493e-02  1.40014648e-01\n",
      "   1.70166016e-01  2.03247070e-02  4.67529297e-02  1.19094849e-02\n",
      "  -1.62231445e-01  1.83593750e-01  5.96313477e-02 -1.39892578e-01\n",
      "  -1.06628418e-01 -2.36816406e-02  1.49414062e-01  1.40014648e-01\n",
      "   1.38626099e-02 -1.49047852e-01  1.06445312e-01 -6.39648438e-02\n",
      "  -2.33520508e-01 -9.42993164e-03  3.92761230e-02  1.41601562e-02\n",
      "   1.51062012e-03 -5.02319336e-02  9.72290039e-02  1.17187500e-01\n",
      "  -6.12487793e-02 -5.13000488e-02 -1.47827148e-01 -3.64685059e-02\n",
      "  -7.61718750e-02  2.10815430e-01 -4.20837402e-02 -9.53979492e-02\n",
      "   4.51660156e-02  3.91006470e-03  1.64184570e-01 -3.99475098e-02\n",
      "  -3.02124023e-02 -1.45385742e-01  1.43310547e-01  5.44433594e-02\n",
      "  -1.19247437e-02 -4.16564941e-02 -3.54614258e-02 -1.16271973e-01\n",
      "  -1.61285400e-02 -1.32080078e-01 -9.49096680e-02  9.89379883e-02\n",
      "  -3.71704102e-02 -6.34002686e-03  1.08215332e-01  4.83703613e-02\n",
      "  -1.17126465e-01 -1.37695312e-01  4.43725586e-02 -9.48486328e-02\n",
      "  -2.01660156e-01  1.03073120e-02 -1.56372070e-01 -4.53186035e-02\n",
      "   8.94775391e-02 -2.61077881e-02 -1.53686523e-01  1.08215332e-01\n",
      "  -4.34265137e-02  1.89056396e-02  1.98120117e-01  6.61621094e-02\n",
      "   2.78125000e+00 -1.59912109e-01  5.52673340e-02 -1.13281250e-01\n",
      "  -5.88378906e-02 -3.78417969e-02 -1.06201172e-02 -6.87866211e-02\n",
      "  -8.00781250e-02 -1.15600586e-01 -3.81469727e-02  4.75883484e-04\n",
      "   7.26928711e-02 -1.74682617e-01 -9.38110352e-02  1.16821289e-01\n",
      "  -4.02221680e-02  9.02709961e-02  1.28540039e-01 -2.38952637e-02\n",
      "  -8.52050781e-02 -7.53784180e-02 -1.31103516e-01  1.01562500e-01\n",
      "  -8.99047852e-02 -1.03378296e-02  1.01135254e-01  4.12597656e-02\n",
      "   2.73590088e-02  4.10461426e-02  9.18579102e-02 -8.62121582e-03\n",
      "  -1.21948242e-01  9.77783203e-02 -1.42089844e-01 -3.36608887e-02\n",
      "  -9.09423828e-03 -6.94580078e-02  2.99072266e-02  1.30371094e-01\n",
      "  -2.28729248e-02  8.69750977e-03 -4.08325195e-02 -3.39965820e-02\n",
      "  -1.42211914e-01 -1.41845703e-01  1.50878906e-01  5.33676147e-03\n",
      "   3.98254395e-02 -3.18717957e-03  2.13867188e-01 -5.76477051e-02\n",
      "  -2.78778076e-02 -1.37252808e-02  3.24058533e-03  2.86865234e-02\n",
      "   1.18530273e-01 -5.60913086e-02 -7.86590576e-03 -5.83496094e-02\n",
      "  -6.35986328e-02 -1.76391602e-01 -5.74874878e-03 -1.03332520e-01\n",
      "  -8.74633789e-02  3.47900391e-02  1.01257324e-01  4.21447754e-02\n",
      "  -2.16979980e-02  1.42211914e-01 -1.77490234e-01  7.40966797e-02\n",
      "   2.60009766e-01  7.32421875e-02 -5.22766113e-02  6.04553223e-02\n",
      "   6.95800781e-02 -3.23791504e-02  9.79614258e-03  1.18225098e-01\n",
      "   9.87243652e-03 -1.72241211e-01  3.45153809e-02  3.48205566e-02\n",
      "   2.90679932e-03  1.72363281e-01  5.48400879e-02 -1.71875000e-01\n",
      "  -7.67211914e-02 -4.16564941e-02 -1.09710693e-02 -3.52783203e-02\n",
      "  -5.41076660e-02  1.29089355e-02  2.35595703e-02 -9.79614258e-03\n",
      "   7.78579712e-03 -6.51855469e-02 -4.95605469e-02 -2.09350586e-01\n",
      "   1.54647827e-02  6.15234375e-02 -7.96508789e-02  1.65271759e-03\n",
      "   1.15112305e-01 -1.90307617e-01  4.13208008e-02  1.95312500e-01\n",
      "   1.45751953e-01 -1.23882294e-03  1.29028320e-01  1.35375977e-01\n",
      "  -1.19873047e-01 -7.76367188e-02  3.91845703e-02  1.69067383e-01\n",
      "  -1.55258179e-02 -7.47070312e-02  3.63159180e-02  5.20019531e-02\n",
      "   4.33044434e-02 -3.02734375e-02 -8.98742676e-03 -8.61358643e-03\n",
      "   4.98046875e-02 -2.03514099e-03 -5.99365234e-02  6.49414062e-02\n",
      "   3.42712402e-02  3.86047363e-02  3.59497070e-02  1.94580078e-01\n",
      "   2.00042725e-02 -9.88159180e-02  6.73828125e-02  4.26940918e-02\n",
      "   4.51965332e-02  2.90069580e-02  5.76019287e-03  1.52206421e-02\n",
      "  -1.69555664e-01  1.86767578e-01 -3.84521484e-02 -4.72412109e-02\n",
      "   1.03271484e-01 -7.47070312e-02 -6.73828125e-02  1.85699463e-02\n",
      "  -8.74633789e-02  1.17309570e-01 -4.45556641e-02 -5.79223633e-02\n",
      "   1.15356445e-02  6.28417969e-01 -1.62231445e-01  9.85717773e-02\n",
      "   4.00543213e-03  1.69799805e-01 -6.98852539e-02  1.90124512e-02\n",
      "   8.99047852e-02 -3.07006836e-02 -6.10656738e-02  2.18627930e-01\n",
      "   3.46069336e-02 -1.14440918e-01  1.37329102e-02 -8.12377930e-02\n",
      "  -8.33740234e-02 -4.30297852e-02  1.37939453e-01  1.79595947e-02\n",
      "   1.31469727e-01 -4.40368652e-02  5.97839355e-02 -7.06787109e-02\n",
      "   4.15649414e-02 -4.36096191e-02  1.18774414e-01 -3.93371582e-02]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "#CUDA_VISIBLE_DEVICES=0\n",
    "#os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "#读取tensorrt引擎\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "import tensorrt as trt\n",
    "logger=trt.Logger(trt.Logger.INFO)\n",
    "runtime=trt.Runtime(logger)\n",
    "trt.init_libnvinfer_plugins(logger,'')\n",
    "with open('engine/BCEembedding_engine','rb') as f:\n",
    "    serialized_engine=f.read()\n",
    "    engine=runtime.deserialize_cuda_engine(serialized_engine)\n",
    "import numpy as np\n",
    "with engine.create_execution_context() as context:\n",
    "    context.set_input_shape('attention_mask', (1,512))\n",
    "    context.set_input_shape('input_ids', (1,512))\n",
    "    input_data=np.arange(1,512,dtype=np.int64)\n",
    "    attention_mask=np.ones((1,512),dtype=np.int64)\n",
    "    last_hidden_state=np.empty((1,512,768),dtype=np.float32)\n",
    "    pooler_output=np.empty((1,768),dtype=np.float32)\n",
    "    d_input_ids=cuda.mem_alloc(input_data.nbytes)\n",
    "    d_input_mask=cuda.mem_alloc(input_data.nbytes)\n",
    "    d_last_hidden_state=cuda.mem_alloc(last_hidden_state.nbytes)\n",
    "    d_pooler_output=cuda.mem_alloc(pooler_output.nbytes)\n",
    "    context.set_tensor_address('input_ids', int(d_input_ids))\n",
    "    context.set_tensor_address('attention_mask',int(d_input_mask))\n",
    "    context.set_tensor_address('last_hidden_state', int(d_last_hidden_state))\n",
    "    context.set_tensor_address('pooler_output', int(d_pooler_output))\n",
    "    stream=cuda.Stream()\n",
    "    cuda.memcpy_htod_async(d_input_ids,input_data,stream)\n",
    "    cuda.memcpy_htod_async(d_input_mask,input_data,stream)\n",
    "    bindings = [int(d_input_ids),int(d_input_mask),int(d_last_hidden_state),int(d_pooler_output)]\n",
    "    context.execute_v2(bindings)\n",
    "    cuda.memcpy_dtoh(last_hidden_state,d_last_hidden_state)\n",
    "\n",
    "print(last_hidden_state[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11/28/2024-08:54:53] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +52, now: CPU 0, GPU 634 (MiB)\n"
     ]
    }
   ],
   "source": [
    "context=engine.create_execution_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "input_data=np.arange(1,512,dtype=np.int64)\n",
    "attention_mask=np.ones((1,512),dtype=np.int64)\n",
    "last_hidden_state=np.empty((1,512,768),dtype=np.float32)\n",
    "pooler_output=np.empty((1,768),dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype=np.float32\n",
    "output_data=np.empty((1,768),dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream=cuda.Stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda.memcpy_htod_async(d_input_ids,input_data,stream)\n",
    "cuda.memcpy_htod_async(d_input_mask,input_data,stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context.set_tensor_address('input_ids', int(d_input_ids))\n",
    "context.set_tensor_address('attention_mask',int(d_input_mask))\n",
    "context.set_tensor_address('last_hidden_state', int(d_last_hidden_state))\n",
    "context.set_tensor_address('pooler_output', int(d_pooler_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bindings = [int(d_input_ids),int(d_input_mask),int(d_last_hidden_state),int(d_pooler_output)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(bindings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m input_ids\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_ids' is not defined"
     ]
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context.set_input_shape(\"input_ids\",(1,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[123813383634944, 123813383635456, 123813383635968, 123813383648256]\n"
     ]
    }
   ],
   "source": [
    "print(bindings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11/28"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/2024-08:57:00] [TRT] [E] IExecutionContext::executeV2: Error Code 1: Myelin ([exec_instruction.cpp:exec:914] CUDA error 400 launching __myl_EqlNotCasTra kernel.)\n"
     ]
    }
   ],
   "source": [
    "context.execute_v2(bindings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cuda' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cuda\u001b[38;5;241m.\u001b[39mmemcpy_dtoh(last_hidden_state,d_last_hidden_state)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(last_hidden_state[:,\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cuda' is not defined"
     ]
    }
   ],
   "source": [
    "cuda.memcpy_dtoh(last_hidden_state,d_last_hidden_state)\n",
    "print(last_hidden_state[:,0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
