{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n将重排序模型BCE转化为tensorrt engine\\n\\n将向量模型转化为tensorrt engine\\n\\n压缩模型可能需要自己实现论文算法与tensorrt引擎进行匹配\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#目前要做的工作\n",
    "\"\"\" \n",
    "将重排序模型BCE转化为tensorrt engine\n",
    "\n",
    "将向量模型转化为tensorrt engine\n",
    "\n",
    "压缩模型可能需要自己实现论文算法与tensorrt引擎进行匹配\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/autodl-tmp/.cache\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "file_dir=os.getcwd()\n",
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "file_dir=os.path.join(file_dir,'../')\n",
    "config.read('../cache.ini')\n",
    "file_name=config['settings']['HF_HOME']\n",
    "file_path=file_name\n",
    "print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/root/miniconda3/lib/python3.8/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XLMRobertaModel(\n",
       "  (embeddings): XLMRobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): XLMRobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x XLMRobertaLayer(\n",
       "        (attention): XLMRobertaAttention(\n",
       "          (self): XLMRobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): XLMRobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): XLMRobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): XLMRobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): XLMRobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION']='python'\n",
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "file_dir=os.getcwd()\n",
    "file_dir=os.path.join(file_dir,'../')\n",
    "config.read('../cache.ini')\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = config['settings']['HF_DATASETS_CACHE']\n",
    "os.environ[\"HF_HOME\"] =config['settings']['HF_HOME']\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = config['settings']['HUGGINGFACE_HUB_CACHE']\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] =config['settings']['TRANSFORMERS_CACHE']\n",
    "os.environ[\"HF_ENDPOINT\"] =config['settings']['HF_ENDPOINT']\n",
    "os.environ[\"XDG_CACHE_HOME\"] = config['settings']['XDG_CACHE_HOME']\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "# list of sentences\n",
    "import torch\n",
    "torch.backends.cuda.enable_flash_sdp(enabled=True)\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(enabled=True)\n",
    "torch.backends.cuda.enable_math_sdp(enabled=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained('maidalun1020/bce-embedding-base_v1',cache_dir=config['settings']['TRANSFORMERS_CACHE'])\n",
    "model = AutoModel.from_pretrained('maidalun1020/bce-embedding-base_v1',torch_dtype=torch.float32,device_map='auto',cache_dir= config['settings']['TRANSFORMERS_CACHE'])\n",
    "model.eval()\n",
    "#device = 'cuda'  # if no GPU, set \"cpu\"\n",
    "#model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMRobertaModel(\n",
       "  (embeddings): XLMRobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): XLMRobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x XLMRobertaLayer(\n",
       "        (attention): XLMRobertaAttention(\n",
       "          (self): XLMRobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): XLMRobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): XLMRobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): XLMRobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): XLMRobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: onnx in /root/miniconda3/lib/python3.8/site-packages (1.14.0)\n",
      "Collecting onnxruntime\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/29/0e/2a5cfe2d7113f4b5c47b51a4dd0a961196cb2cf72d6f1952de36ba1e3147/onnxruntime-1.19.2-cp38-cp38-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.2 MB 6.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pycuda\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/61/69/f53a6624def08348778a7407683f44c2a9adfdb0b68b9a45f8213ff66c9d/pycuda-2024.1.2.tar.gz (1.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.7 MB 3.4 MB/s eta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.20.2 in /root/miniconda3/lib/python3.8/site-packages (from onnx) (4.22.1)\n",
      "Requirement already satisfied: numpy in /root/miniconda3/lib/python3.8/site-packages (from onnx) (1.24.2)\n",
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in /root/miniconda3/lib/python3.8/site-packages (from onnx) (4.5.0)\n",
      "Collecting flatbuffers\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/41/f0/7e988a019bc54b2dbd0ad4182ef2d53488bb02e58694cd79d61369e85900/flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: sympy in /root/miniconda3/lib/python3.8/site-packages (from onnxruntime) (1.11.1)\n",
      "Requirement already satisfied: packaging in /root/miniconda3/lib/python3.8/site-packages (from onnxruntime) (23.0)\n",
      "Collecting coloredlogs\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/a7/06/3d6badcf13db419e25b07041d9c7b4a2c331d3f4e7134445ec5df57714cd/coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[K     |████████████████████████████████| 46 kB 33.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: platformdirs>=2.2.0 in /root/miniconda3/lib/python3.8/site-packages (from pycuda) (3.1.1)\n",
      "Collecting pytools>=2011.2\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/70/19/87514026ff33ae67681e7e721872db8d34fd0fc25ec28906fb7b1e5c57d0/pytools-2024.1.14-py3-none-any.whl (89 kB)\n",
      "\u001b[K     |████████████████████████████████| 89 kB 4.5 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting mako\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/1e/bf/7a6a36ce2e4cafdfb202752be68850e22607fccd692847c45c1ae3c17ba6/Mako-1.3.8-py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 60.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting humanfriendly>=9.1\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/f0/0f/310fb31e39e2d734ccaa2c0fb981ee41f7bd5056ce9bc29b2248bd569169/humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[K     |████████████████████████████████| 86 kB 34.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.9.2 in /root/miniconda3/lib/python3.8/site-packages (from mako->pycuda) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /root/miniconda3/lib/python3.8/site-packages (from sympy->onnxruntime) (1.3.0)\n",
      "Building wheels for collected packages: pycuda\n",
      "  Building wheel for pycuda (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pycuda: filename=pycuda-2024.1.2-cp38-cp38-linux_x86_64.whl size=664247 sha256=ec2d2d170f99103297fedfa5daa6556273b913459b6b05f889baabeff587ccfb\n",
      "  Stored in directory: /root/autodl-tmp/.cache/pip/wheels/d2/41/89/5470e8340bbb6411e64272cc830b52e6d32f2f069a12b9061e\n",
      "Successfully built pycuda\n",
      "Installing collected packages: humanfriendly, pytools, mako, flatbuffers, coloredlogs, pycuda, onnxruntime\n",
      "Successfully installed coloredlogs-15.0.1 flatbuffers-24.3.25 humanfriendly-10.0 mako-1.3.8 onnxruntime-1.19.2 pycuda-2024.1.2 pytools-2024.1.14\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install onnx onnxruntime pycuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt as trt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cuda\"\n",
    "def embedding(text_list):\n",
    "    inputs = tokenizer(text_list, padding=True, truncation=True, max_length=520, return_tensors=\"pt\")\n",
    "    inputs_on_device = {k: v.to(device) for k, v in inputs.items()}\n",
    "    # get embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs_on_device, return_dict=True)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs=embedding([\"你好，好久不见\"])\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 768])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0, 35378,     4,   759, 10269,    83, 99942,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer([\"Hello, my dog is cute\"], padding=True, truncation=True, max_length=520, return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.0+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 将 PyTorch 模型转化为 ONNX 引擎\n",
    "# 1、定义输入张量的形状信息\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# 创建输入张量\n",
    "input_id_ = torch.randint(2, 1000, (1, 512),dtype=torch.int64).to('cuda')  # 在 GPU 上创建 input_ids\n",
    "attention_mask_ = torch.ones((1, 512), dtype=torch.int64).to('cuda')  # 正确创建 attention_mask 并转到 GPU\n",
    "\n",
    "# 转化模型\n",
    "torch.onnx.export(\n",
    "    model,  # 原模型\n",
    "    (input_id_, attention_mask_),  # 输入张量，接受一个张量或者元组\n",
    "    \"/root/autodl-tmp/tensorrt/BCEembedding.onnx\",\n",
    "    export_params=True,  # 是否保存模型的权重信息\n",
    "    opset_version=15,  # 17支持 INormalizationLayer，防止溢出\n",
    "    do_constant_folding=True,  # 是否执行常量折叠优化\n",
    "    input_names=['input_ids', 'attention_mask'],  # 输入的名字\n",
    "    output_names=['last_hidden_state', 'pooler_output'],  # 输出的名字\n",
    "    dynamic_axes={\n",
    "        'input_ids': {0: 'batch_size', 1: 'sequence_length'},\n",
    "        'attention_mask': {0: 'batch_size', 1: 'sequence_length'},\n",
    "        'last_hidden_state': {0: 'batch_size', 1: 'sequence_length'},\n",
    "        'pooler_output': {0: 'batch_size'},\n",
    "    }  # 可变长度，在 NLP 中批次和序列长度都是可变长度\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LD_LIBRARY_PATH: /usr/local/lib/tensorrt:\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# 设置 LD_LIBRARY_PATH\n",
    "os.environ['LD_LIBRARY_PATH'] = '/usr/local/lib/tensorrt:' + os.environ.get('LD_LIBRARY_PATH', '')\n",
    "\n",
    "# 打印当前的 LD_LIBRARY_PATH 以验证\n",
    "print(\"LD_LIBRARY_PATH:\", os.environ['LD_LIBRARY_PATH'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorrt as trt\n",
    "logger=trt.Logger(trt.Logger.WARNING)\n",
    "trt.init_libnvinfer_plugins(logger,namespace='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/13/2024-22:46:42] [TRT] [E] 6: [libLoader.cpp::Impl::293] Error Code 6: Internal Error (Unable to load library: libnvinfer_builder_resource.so.8.6.1: libnvinfer_builder_resource.so.8.6.1: cannot open shared object file: No such file or directory)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "pybind11::init(): factory function returned nullptr",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m builder\u001b[38;5;241m=\u001b[39m\u001b[43mtrt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBuilder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: pybind11::init(): factory function returned nullptr"
     ]
    }
   ],
   "source": [
    "\n",
    "builder=trt.Builder(logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'builder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m config\u001b[38;5;241m=\u001b[39m\u001b[43mbuilder\u001b[49m\u001b[38;5;241m.\u001b[39mcreate_builder_config()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'builder' is not defined"
     ]
    }
   ],
   "source": [
    "config=builder.create_builder_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.set_flag(trt.BuilderFlag.FP16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/10/2024-11:03:03] [TRT] [W] The implicit batch dimension mode has been deprecated. Please create the network with NetworkDefinitionCreationFlag::kEXPLICIT_BATCH flag whenever possible.\n"
     ]
    }
   ],
   "source": [
    "network = builder.create_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "network=builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile = builder.create_optimization_profile()\n",
    "profile.set_shape(\"input_ids\", (1, 1),(4, 256),(8, 512))  # 输入的最小、默认批量大小、最大批次\n",
    "profile.set_shape(\"attention_mask\",(1,1),(4,256),(8,512))\n",
    "config.add_optimization_profile(profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE,1<<30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser=trt.OnnxParser(network,logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[libprotobuf WARNING google/protobuf/io/coded_stream.cc:604] Reading dangerously large protocol message.  If the message turns out to be larger than 2147483647 bytes, parsing will be halted for security reasons.  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.\n",
      "[libprotobuf WARNING google/protobuf/io/coded_stream.cc:81] The total number of bytes read was 1112412160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/13/2024-14:04:14] [TRT] [W] onnx2trt_utils.cpp:374: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "[12/13/2024-14:04:14] [TRT] [W] onnx2trt_utils.cpp:400: One or more weights outside the range of INT32 was clamped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[libprotobuf WARNING google/protobuf/io/coded_stream.cc:604] Reading dangerously large protocol message.  If the message turns out to be larger than 2147483647 bytes, parsing will be halted for security reasons.  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.\n",
      "[libprotobuf WARNING google/protobuf/io/coded_stream.cc:81] The total number of bytes read was 1112412160\n"
     ]
    }
   ],
   "source": [
    "success=parser.parse_from_file('engine/BCEembedding.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(success)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(parser.num_errors):\n",
    "    print(parser.get_error(idx))\n",
    "if not success:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/autodl-tmp/3.0/Tensorrt/resources/../../Engine/model/.cache\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/13/2024-14:05:04] [TRT] [W] TensorRT encountered issues when converting weights between types and that could affect accuracy.\n",
      "[12/13/2024-14:05:04] [TRT] [W] If this is not the desired behavior, please modify the weights or retrain with regularization to adjust the magnitude of the weights.\n",
      "[12/13/2024-14:05:04] [TRT] [W] Check verbose logs for the list of affected weights.\n",
      "[12/13/2024-14:05:04] [TRT] [W] - 118 weights are affected by this issue: Detected subnormal FP16 values.\n",
      "[12/13/2024-14:05:04] [TRT] [W] - 46 weights are affected by this issue: Detected values less than smallest positive FP16 subnormal value and converted them to the FP16 minimum subnormalized value.\n",
      "[12/13/2024-14:05:04] [TRT] [W] - 1 weights are affected by this issue: Detected finite FP32 values which would overflow in FP16 and converted them to the closest finite FP16 value.\n"
     ]
    }
   ],
   "source": [
    "serialized_engine=builder.build_serialized_network(network,config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('engine/BCEembedding_engine','wb') as f:\n",
    "          f.write(serialized_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/13/2024-22:49:13] [TRT] [I] Loaded engine size: 1063 MiB\n",
      "[12/13/2024-22:49:13] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +1060, now: CPU 0, GPU 1060 (MiB)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#读取tensorrt引擎\n",
    "import tensorrt as trt\n",
    "logger=trt.Logger(trt.Logger.INFO)\n",
    "runtime=trt.Runtime(logger)\n",
    "trt.init_libnvinfer_plugins(logger,'')\n",
    "with open('/root/autodl-tmp/tensorrt/BCEembedding_engine','rb') as f:\n",
    "    serialized_engine=f.read()\n",
    "    engine=runtime.deserialize_cuda_engine(serialized_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor Name:input_ids,Shape:(-1, -1),Size:1,Data Type:<class 'numpy.int32'>\n",
      "Tensor Name:attention_mask,Shape:(-1, -1),Size:1,Data Type:<class 'numpy.int32'>\n",
      "Tensor Name:last_hidden_state,Shape:(-1, -1, 768),Size:768,Data Type:<class 'numpy.float16'>\n",
      "Tensor Name:pooler_output,Shape:(-1, 768),Size:-768,Data Type:<class 'numpy.float16'>\n"
     ]
    }
   ],
   "source": [
    "for i in range(engine.num_io_tensors):\n",
    "    tensor_name=engine.get_tensor_name(i)\n",
    "    tensor_shape=engine.get_tensor_shape(tensor_name)\n",
    "    size=trt.volume(tensor_shape)\n",
    "    dtype=trt.nptype(engine.get_tensor_dtype(tensor_name))\n",
    "    \n",
    "    print(f\"Tensor Name:{tensor_name},Shape:{tensor_shape},Size:{size},Data Type:{dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "input_data=np.arange(1,4,dtype=np.int64)\n",
    "attention_mask=np.ones((1,4),dtype=np.int64)\n",
    "last_hidden_state=np.empty((1,4,768),dtype=np.float32)\n",
    "pooler_output=np.empty((1,768),dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"你好\", padding=True, truncation=True, max_length=520,return_tensors=\"pt\")\n",
    "inputs['input_ids'].device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/13/2024-22:49:20] [TRT] [I] The logger passed into createInferRuntime differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value.\n",
      "[12/13/2024-22:49:20] [TRT] [I] Loaded engine size: 1063 MiB\n",
      "[12/13/2024-22:49:21] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +1061, now: CPU 0, GPU 2121 (MiB)\n",
      "[12/13/2024-22:49:21] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +264, now: CPU 0, GPU 1324 (MiB)\n",
      "[[ 2.59577138e-10  1.87145720e-12 -3.03612580e-09 -1.21406249e-14\n",
      "  -4.98715416e-08 -7.25331128e-09 -1.67869275e-14  1.20777997e-07\n",
      "  -4.52302605e-12 -3.92382049e-07 -2.56023640e-11 -2.22101726e-10\n",
      "  -4.26868913e-12 -1.03703741e-07 -1.00348547e-17  6.68428117e-12\n",
      "   1.75719203e-10 -5.01042940e-08 -1.76718944e-19  9.13882969e-10\n",
      "   5.62281510e-09  2.68590732e-08 -2.73229901e-08  1.38799380e-11\n",
      "  -3.64444702e-07 -1.03237053e-05  1.29041791e-06  2.16296360e-12\n",
      "   1.12366954e-19  4.25311509e-06 -3.41524864e-11  2.50753118e-07\n",
      "  -2.22303896e-07 -8.69297039e-13 -1.94830201e-17  1.56174185e-09\n",
      "   1.31488520e-07 -2.37549077e-15 -3.82284497e-08  1.64858809e-20\n",
      "   2.34416306e-11  4.48751303e-09  3.96930545e-06 -6.52922712e-19\n",
      "   1.43174539e-05  1.39272051e-05  3.34726991e-07 -1.30192745e-10\n",
      "   1.04172115e-09 -2.82175654e-11 -1.27984565e-02 -1.00824017e-14\n",
      "  -5.72531311e-09 -1.32146879e-05 -8.07110729e-19  1.99283878e-08\n",
      "  -2.72169564e-06  6.24895169e-14  2.03784020e-28 -9.96354265e-06\n",
      "   1.03462161e-09  1.77607088e-21 -3.17762504e-16  5.59227900e-08\n",
      "  -1.54774256e-07 -9.99813210e-07 -5.93055249e-12  7.82877513e-11\n",
      "  -1.01257280e-10 -6.83146514e-15 -2.34966734e-11  7.59575935e-17\n",
      "  -6.87246618e-04  1.06723970e-10 -4.64807792e-09 -9.64807185e-08\n",
      "   2.95263841e-13  1.65711195e-10  2.72265099e-10  1.12764099e-14\n",
      "   6.62703573e-12  1.86449100e-07 -3.39586893e-13  8.03627239e-13\n",
      "   4.48327086e-07  9.57949831e-11 -1.04007700e-08 -6.76155937e-11\n",
      "  -3.77457042e-07 -3.55652560e-06  6.21606152e-12  3.58426412e-16\n",
      "  -1.42868585e-05 -1.56928970e-09  7.97919809e-14 -5.75531338e-08\n",
      "   5.14812802e-12 -2.28333008e-07  1.54018217e-05 -2.31181220e-12\n",
      "  -8.10581853e-13  1.79951229e-07 -3.17013473e-04 -2.03368765e-14\n",
      "   7.00567071e-09 -1.30865490e-12 -4.27888037e-16  4.73716032e-12\n",
      "   4.69661582e-13  5.32442562e-11  7.18195208e-12  1.50127264e-07\n",
      "   1.95765821e-07  3.03695112e-12 -7.75529529e-10  8.68715972e-07\n",
      "   1.81665072e-09  1.44734225e-12 -5.08086628e-10 -2.82506445e-08\n",
      "  -1.36192935e-09 -4.14131941e-15  4.34560530e-14 -7.59973855e-12\n",
      "   2.80715057e-07 -1.48571676e-11  2.58578381e-09 -2.68544142e-07\n",
      "   5.92058208e-11 -1.50606265e-05 -9.22944166e-10  6.58758741e-04\n",
      "  -1.93430001e-08  7.30774809e-06 -1.71910715e-11  7.48274109e-10\n",
      "   6.25692640e-12  4.00086780e-15 -1.34772819e-07 -1.10900714e-07\n",
      "   8.59427018e-06 -6.11568427e-14 -3.91842820e-15  2.09665154e-06\n",
      "  -1.09680026e-14 -1.00660394e-07  3.37770420e-13 -4.35869012e-08\n",
      "  -1.24398512e-05  6.91809134e-07  5.98440097e-09 -4.94567122e-15\n",
      "   3.62845676e-05 -1.58904400e-16 -9.95225591e-08 -1.47562919e-12\n",
      "   2.55526658e-07  7.74044162e-11  4.06420440e-07  2.27972752e-09\n",
      "   1.16639051e-11 -8.60047464e-08 -6.39617554e-07  1.15354150e-10\n",
      "   2.70252679e-12  2.82548402e-08  6.38415416e-17 -2.60423079e-08\n",
      "  -1.31619355e-08  2.97174041e-09  7.48540119e-12 -1.49182426e-07\n",
      "  -1.83432505e-10 -4.97679480e-07 -3.99911841e-07  2.14388777e-16\n",
      "   5.46499109e-07  3.13651123e-12  1.21728505e-07  1.73385877e-06\n",
      "   5.66667210e-14  3.30490701e-09 -3.54922622e-06 -1.91562738e-17\n",
      "   2.20227864e-11 -1.35151588e-08 -2.06622059e-13  3.75968945e-09\n",
      "  -1.08160646e-06  5.68753478e-07  1.02534791e-07 -4.53001832e-07\n",
      "  -2.58292518e-21 -5.63808499e-06  4.88243557e-10  4.06924451e-12\n",
      "  -1.21476456e-08 -5.02108364e-08  1.08168831e-06 -1.45348080e-08\n",
      "   2.41649386e-14  1.50033075e-09 -8.26675023e-06 -3.44750139e-23\n",
      "   1.35148666e-08 -5.23379659e-14 -1.25848869e-14  1.20319124e-08\n",
      "   1.96004546e-09 -3.19854143e-11  1.10048370e-06 -4.34655992e-11\n",
      "   9.50710728e-07 -2.02484252e-09 -1.31415963e-11 -2.10000660e-21\n",
      "   1.38980534e-28  3.00853671e-08  4.29787095e-09  7.14798637e-11\n",
      "  -1.01974003e-08  8.69874292e-15  2.89478148e-08  7.46025768e-19\n",
      "   4.99635089e-09 -2.59737436e-13 -7.40279664e-17 -4.49796531e-13\n",
      "   9.44172829e-09 -5.78597719e-05  1.22110320e-23  3.48049878e-09\n",
      "  -1.86237202e-13  2.40051463e-08 -7.34200043e-08 -8.82979356e-10\n",
      "  -2.49023174e-07  2.79015673e-08  1.31905225e-08 -1.36137324e-10\n",
      "  -2.51708574e-08  2.18360266e-14 -3.00858969e-12  4.61363641e-07\n",
      "  -1.35236086e-10  1.49791388e-10  2.23174429e-05 -3.59643669e-11\n",
      "  -2.61781543e-06 -2.88982692e-11 -7.94100636e-12  1.36590179e-08\n",
      "  -1.25326926e-06  9.17700635e-14 -1.98060320e-07 -2.54049404e-08\n",
      "   1.64922689e-08  8.24806206e-13 -7.79197318e-10 -3.77351039e-09\n",
      "   2.16430538e-13 -5.78190585e-14 -2.91230613e-08 -5.28062128e-06\n",
      "   9.34433970e-08 -1.22559248e-21 -2.02221759e-02  5.20148191e-09\n",
      "  -9.08460251e-10 -1.37028664e-07 -3.75684067e-11 -6.81426426e-09\n",
      "  -1.02465480e-15  1.28748106e-08  3.84922349e-15 -2.35327263e-07\n",
      "   7.09894673e-17  4.16040258e-10 -2.81062618e-09  3.98263818e-11\n",
      "  -1.79864823e-09 -1.33604514e-27 -3.78498367e-16  2.13281541e-14\n",
      "   5.57323798e-10 -6.71963651e-10 -9.47257384e-10  1.28172140e-08\n",
      "   4.74940562e-13 -1.75236721e-06  5.59051152e-08  1.98901548e-10\n",
      "   1.10400881e-11 -2.09337297e-10 -9.85213055e-18 -1.76023036e-08\n",
      "   3.88280389e-19  1.31171156e-13 -2.68499025e-06  2.75287770e-09\n",
      "  -1.67382022e-15  1.62506185e-12  4.06246287e-11  3.27719007e-09\n",
      "  -4.14145218e-10 -2.50633525e-06 -5.22120238e-12  1.91038112e-06\n",
      "   1.34756249e-07  1.23124544e-09 -7.97738086e-15 -1.53612973e-06\n",
      "  -3.28495132e-15 -7.93441332e-16  5.50670169e-11 -8.87771592e-17\n",
      "   9.90307782e-08  1.42440925e-18  1.14123930e-06  4.08605638e-19\n",
      "   3.00060088e-09  2.68533355e-11 -3.43026090e-06  2.45856883e-20\n",
      "  -5.22891301e-13 -1.77996112e-16  3.46599194e-09  5.29994126e-13\n",
      "   3.51968694e-08 -2.90125719e-11  3.79593682e-13 -5.09968068e-10\n",
      "   4.29642516e-10  2.01186046e-14 -3.82236379e-07  3.27736366e-10\n",
      "   1.01270536e-09  1.08455020e-13  2.84105350e-10 -5.68121670e-22\n",
      "  -2.89485840e-12 -1.67871583e-08  8.52033943e-10  1.69647030e-09\n",
      "   6.49229409e-14 -8.24796546e-17 -4.10293242e-19 -4.50704822e-13\n",
      "  -1.90351839e-05  7.15960937e-07  1.25541667e-12 -7.53672125e-10\n",
      "   3.97352236e-13 -6.85960089e-09 -1.45844078e-05  6.79689161e-12\n",
      "   3.55449608e-08  2.06671757e-06 -8.12068746e-10  8.16874932e-15\n",
      "   2.22090138e-10 -9.96435989e-11  3.64071218e-10  1.33388740e-08\n",
      "   1.43288395e-16  1.61612085e-10  1.47057075e-10  2.02886985e-15\n",
      "   1.39725157e-14 -2.02844487e-04  5.97690741e-05 -1.50390399e-12\n",
      "   5.17828141e-12 -6.10286355e-09 -1.34823350e-11  6.81053955e-13\n",
      "   5.33098185e-16  2.81003655e-13  7.90036312e-16  2.56449397e-07\n",
      "   3.60113347e-14  2.34796129e-08  2.84472250e-12  2.86286538e-13\n",
      "   2.83866663e-11  2.59532951e-10 -1.08983905e-10  2.97389282e-21\n",
      "  -1.32553959e-11 -3.02988190e-12  3.53233748e-10  4.18345678e-08\n",
      "  -1.76448930e-13 -4.74868411e-09  9.02508746e-19 -7.28544474e-22\n",
      "  -2.86534067e-16 -8.97228389e-08 -6.94410400e-15  5.97518346e-13\n",
      "   2.76476168e-11 -1.55052167e-08  2.56759631e-12  4.06975972e-12\n",
      "   7.86284371e-10  7.44733442e-10 -3.04907701e-07  4.23178076e-10\n",
      "  -2.25995684e-08 -1.42670700e-07  2.38778398e-07  1.76605022e-10\n",
      "  -8.17856525e-14  3.55511247e-08 -4.89309149e-11  7.42795603e-10\n",
      "  -1.22642618e-07 -3.12272458e-10 -4.54468819e-14  1.80907995e-21\n",
      "   1.18813460e-07 -4.21313944e-13 -4.28284030e-09 -3.24500545e-13\n",
      "   4.41068839e-12  3.21016512e-11  2.27773977e-08  8.96421132e-12\n",
      "   2.53683737e-07  6.50281208e-07  2.81344548e-08 -1.58729452e-09\n",
      "   5.40518628e-12 -3.75690249e-14 -4.55998039e-09  7.30858196e-16\n",
      "  -1.24207067e-09 -3.40312567e-07 -2.13807262e-17  4.76707729e-10\n",
      "  -1.74776233e-07  3.42372454e-15  1.15302053e-15 -6.74846945e-07\n",
      "   3.66501482e-11 -1.96763644e-18  2.08415289e-13  1.62056743e-10\n",
      "  -2.37432496e-09 -3.15022008e-10 -7.44472151e-10 -7.08160397e-12\n",
      "  -8.01703051e-13 -2.43221675e-12 -5.41934856e-12  8.53414821e-12\n",
      "  -8.35228414e-11  2.12884316e-13 -8.61084370e-10 -4.78018189e-12\n",
      "  -6.31887704e-17  1.93480041e-11  3.33496251e-12  2.62416391e-12\n",
      "   9.57968704e-11  6.19151175e-10 -2.84593414e-14  2.21327053e-13\n",
      "   5.17133225e-09  3.66735315e-14 -6.80697512e-11 -1.28825929e-15\n",
      "  -3.67643249e-09 -1.80071538e-06  1.42849028e-22  5.88119553e-10\n",
      "  -7.40090854e-07 -1.08210038e-13 -2.73579163e-11 -5.24539689e-10\n",
      "   3.38790851e-14 -8.51061582e-11  6.97338407e-07 -1.08544007e-10\n",
      "  -7.03837336e-12  9.33412549e-12 -4.29596075e-05 -2.03980320e-13\n",
      "   3.39601125e-10  1.50892716e-17 -5.26127969e-12 -1.74382122e-18\n",
      "   1.60629131e-16 -7.34141137e-16  1.70155071e-11  2.01101047e-09\n",
      "   4.74958295e-09  1.76167594e-10 -4.16679047e-09  6.37809583e-09\n",
      "   3.82381071e-11 -1.91071681e-15 -2.06997462e-12  5.80163223e-14\n",
      "   4.69055117e-25 -1.33401021e-13 -3.39482150e-16  5.18126200e-15\n",
      "   6.31314337e-13  1.40037585e-13  3.83222509e-10 -2.26141064e-10\n",
      "  -1.67138325e-21 -3.32220367e-08 -2.25709859e-13  3.03524284e-05\n",
      "  -2.40588420e-08  5.80168944e-08 -1.28902228e-14  1.74397841e-09\n",
      "   3.01229824e-16 -4.26626812e-14 -6.95487808e-12  2.11086834e-22\n",
      "   1.68039343e-13  2.25903411e-11 -7.52639038e-21  4.79945389e-11\n",
      "  -1.18341248e-11 -1.62027352e-08 -1.69740677e-29 -2.95968111e-11\n",
      "  -7.29055273e-07  1.44075585e-07  6.82590399e-12 -1.17761738e-11\n",
      "   1.47840096e-09  1.01029338e-10 -6.24881658e-09 -1.94069561e-19\n",
      "   1.99455670e-07  7.33046124e-11  1.49189574e-07  6.85839613e-16\n",
      "  -1.14605308e-21 -9.86425009e-15 -1.52140789e-08  7.46260097e-18\n",
      "  -1.18055397e-15  7.01178698e-12  3.67808756e-10 -1.39515430e-08\n",
      "  -2.95643443e-09  6.18900601e-13  1.37954786e-10 -3.37890624e-14\n",
      "  -1.96346228e-08 -5.81163455e-08 -7.48221609e-08 -6.40942965e-10\n",
      "   1.27100297e-09  5.08941864e-17  3.28885530e-11  8.42277537e-09\n",
      "  -3.36072912e-13  5.08039499e-10 -3.05812449e-07  3.05072981e-12\n",
      "   2.73641405e-11 -1.58428604e-10  3.12784623e-19  2.51317384e-10\n",
      "  -1.51820125e-12  1.44555945e-09  2.03187966e-07 -4.48735360e-09\n",
      "   2.94355137e-18 -4.36184791e-07  6.56554485e-25  3.87003971e-12\n",
      "  -1.97411421e-09 -9.61445679e-09  1.28915956e-09 -5.16391487e-12\n",
      "   2.45464795e-14  3.40775541e-09 -1.76336960e-06 -1.16409830e-14\n",
      "   3.15933704e-07  5.64776698e-22 -7.25543689e-15  1.02301179e-07\n",
      "   3.50034942e-14 -1.19190552e-11  4.22249457e-07 -1.07839453e-10\n",
      "   7.50620472e-08 -7.06349257e-10 -3.53430257e-14 -5.40047243e-16\n",
      "   2.38909326e-20  9.52076362e-16  5.28914725e-11  2.33185166e-10\n",
      "  -1.15615999e-13  2.29795046e-12  1.98567239e-11 -5.31997103e-12\n",
      "   2.33814501e-09 -3.32389424e-19  1.99735454e-11 -1.02010422e-09\n",
      "   4.71100797e-11  1.17023599e-10 -1.16601397e-21  2.14189266e-09\n",
      "   2.26142381e-14  1.24505766e-13 -5.04042186e-09 -1.77012884e-11\n",
      "  -6.43628795e-09  1.84573556e-09  2.16205526e-11 -1.45282156e-13\n",
      "  -6.04620687e-10 -5.87408507e-12  3.39034797e-14  5.05488629e-09\n",
      "  -2.49399415e-14  2.54186605e-09  1.43305785e-08 -2.67401097e-12\n",
      "  -3.29879879e-09 -1.10210895e-20 -1.34162412e-14  2.97815062e-09\n",
      "  -6.01554575e-08  1.88914111e-13 -2.62912847e-09 -5.69184619e-13\n",
      "   1.52549869e-12  2.79868056e-11 -3.52791234e-11 -2.71190703e-13\n",
      "   8.67848762e-12 -1.19217002e-20 -1.21989207e-11 -4.41781651e-07\n",
      "   1.97046018e-10  2.82155480e-11 -2.39462606e-04  9.43740419e-10\n",
      "  -4.53927087e-12 -2.44051418e-10 -9.90285545e-12 -9.66712858e-15\n",
      "   1.95241179e-15  6.55485499e-10  1.14489458e-08 -5.34787787e-08\n",
      "  -1.89504818e-14 -5.67778890e-14 -1.84789295e-10  9.67593952e-13\n",
      "  -3.20474147e-10  8.80942980e-15  5.12051501e-17  3.12348617e-14\n",
      "   2.32019773e-15 -9.81667959e-12 -8.06444760e-08  1.24895960e-09\n",
      "   2.05290571e-16 -5.28180610e-10  1.11517531e-11  1.70180606e-11\n",
      "   2.88321816e-19 -1.02987332e-11 -2.13946597e-10 -1.30366506e-09\n",
      "   3.52842989e-18  2.94288411e-13 -8.85141702e-08  5.90409509e-26\n",
      "  -1.38246402e-11 -1.69334062e-26  1.13452845e-12  1.08560594e-09\n",
      "  -1.87066682e-10 -3.15020657e-07 -3.67094381e-19  1.26739097e-09\n",
      "   4.27687539e-08  6.31546595e-12 -6.86751297e-19 -1.04677178e-11\n",
      "   2.67671925e-13  1.75722651e-18  8.84497011e-14  1.01793561e-15\n",
      "   7.64195666e-15 -3.33350278e-23  9.90556881e-09 -2.63176351e-12\n",
      "   6.66989841e-09 -1.41541694e-12 -3.47370808e-08  7.79364450e-25\n",
      "  -4.73331166e-11 -7.75191868e-13  2.02548377e-11  1.01690498e-14\n",
      "   1.19428722e-09 -4.17778984e-13  7.82377838e-14 -5.86723270e-13\n",
      "   7.18801265e-16  7.28448968e-11 -2.17193488e-07  7.39826603e-12\n",
      "   1.16039409e-11 -2.20454121e-13  7.77981060e-14 -1.58000348e-15\n",
      "  -4.37705461e-15 -1.10581981e-10  5.96578620e-11 -2.30593603e-15\n",
      "  -4.44731420e-16 -6.61228407e-12 -4.47994642e-13 -1.60372887e-12\n",
      "  -1.62630613e-07  8.56605009e-09  3.77131963e-12 -7.39942569e-12\n",
      "   2.30586441e-14 -5.79107630e-11 -2.52660158e-07 -1.44152313e-20\n",
      "   2.84987693e-11  2.95597488e-07 -6.33052014e-11  7.98727334e-11\n",
      "   4.08442844e-18  5.17119569e-10  4.62215715e-14  2.52653365e-09\n",
      "   2.27539969e-16  5.51854340e-10  3.49423442e-11  1.41525756e-12\n",
      "   1.25381819e-15 -2.02729169e-07  1.20012250e-04  2.01280053e-23\n",
      "   4.11419565e-10 -2.11119328e-07 -3.53223895e-10  5.37299927e-10\n",
      "  -4.11043081e-19 -1.63187875e-15  9.22459511e-17  2.97612672e-08\n",
      "   1.21627816e-12  6.48108811e-09  1.96597469e-15 -4.15492987e-12]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "#CUDA_VISIBLE_DEVICES=0\n",
    "#os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "#读取tensorrt引擎\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "import tensorrt as trt\n",
    "logger=trt.Logger(trt.Logger.INFO)\n",
    "runtime=trt.Runtime(logger)\n",
    "trt.init_libnvinfer_plugins(logger,'')\n",
    "with open('/root/autodl-tmp/tensorrt/BCEembedding_engine','rb') as f:\n",
    "    serialized_engine=f.read()\n",
    "    engine=runtime.deserialize_cuda_engine(serialized_engine)\n",
    "import numpy as np\n",
    "with engine.create_execution_context() as context:\n",
    "    context.set_input_shape('attention_mask', (1,512))\n",
    "    context.set_input_shape('input_ids', (1,512))\n",
    "    input_data=np.arange(1,512,dtype=np.int64)\n",
    "    attention_mask=np.ones((1,512),dtype=np.int64)\n",
    "    last_hidden_state=np.empty((1,512,768),dtype=np.float32)\n",
    "    pooler_output=np.empty((1,768),dtype=np.float32)\n",
    "    d_input_ids=cuda.mem_alloc(input_data.nbytes)\n",
    "    d_input_mask=cuda.mem_alloc(input_data.nbytes)\n",
    "    d_last_hidden_state=cuda.mem_alloc(last_hidden_state.nbytes)\n",
    "    d_pooler_output=cuda.mem_alloc(pooler_output.nbytes)\n",
    "    context.set_tensor_address('input_ids', int(d_input_ids))\n",
    "    context.set_tensor_address('attention_mask',int(d_input_mask))\n",
    "    context.set_tensor_address('last_hidden_state', int(d_last_hidden_state))\n",
    "    context.set_tensor_address('pooler_output', int(d_pooler_output))\n",
    "    stream=cuda.Stream()\n",
    "    cuda.memcpy_htod_async(d_input_ids,input_data,stream)\n",
    "    cuda.memcpy_htod_async(d_input_mask,input_data,stream)\n",
    "    bindings = [int(d_input_ids),int(d_input_mask),int(d_last_hidden_state),int(d_pooler_output)]\n",
    "    context.execute_v2(bindings)\n",
    "    cuda.memcpy_dtoh(last_hidden_state,d_last_hidden_state)\n",
    "\n",
    "print(last_hidden_state[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11/28/2024-08:54:53] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +52, now: CPU 0, GPU 634 (MiB)\n"
     ]
    }
   ],
   "source": [
    "context=engine.create_execution_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "input_data=np.arange(1,512,dtype=np.int64)\n",
    "attention_mask=np.ones((1,512),dtype=np.int64)\n",
    "last_hidden_state=np.empty((1,512,768),dtype=np.float32)\n",
    "pooler_output=np.empty((1,768),dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype=np.float32\n",
    "output_data=np.empty((1,768),dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream=cuda.Stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda.memcpy_htod_async(d_input_ids,input_data,stream)\n",
    "cuda.memcpy_htod_async(d_input_mask,input_data,stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context.set_tensor_address('input_ids', int(d_input_ids))\n",
    "context.set_tensor_address('attention_mask',int(d_input_mask))\n",
    "context.set_tensor_address('last_hidden_state', int(d_last_hidden_state))\n",
    "context.set_tensor_address('pooler_output', int(d_pooler_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bindings = [int(d_input_ids),int(d_input_mask),int(d_last_hidden_state),int(d_pooler_output)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(bindings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m input_ids\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_ids' is not defined"
     ]
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context.set_input_shape(\"input_ids\",(1,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[123813383634944, 123813383635456, 123813383635968, 123813383648256]\n"
     ]
    }
   ],
   "source": [
    "print(bindings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11/28"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/2024-08:57:00] [TRT] [E] IExecutionContext::executeV2: Error Code 1: Myelin ([exec_instruction.cpp:exec:914] CUDA error 400 launching __myl_EqlNotCasTra kernel.)\n"
     ]
    }
   ],
   "source": [
    "context.execute_v2(bindings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cuda' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cuda\u001b[38;5;241m.\u001b[39mmemcpy_dtoh(last_hidden_state,d_last_hidden_state)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(last_hidden_state[:,\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cuda' is not defined"
     ]
    }
   ],
   "source": [
    "cuda.memcpy_dtoh(last_hidden_state,d_last_hidden_state)\n",
    "print(last_hidden_state[:,0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
