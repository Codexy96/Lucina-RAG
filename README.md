# Lucina项目 RAG框架 README

## 项目概述

Lucina项目旨在建立一个高效的RAG（Retrieval-Augmented Generation）系统，融合多种检索和生成技术，以便为用户提供更准确、更快速的问答服务。

## 技术选型如下：

- 向量搜索：
  - milvus
- 向量模型：
  - embedding-3
- 全文检索：
  - Elasticsearch
- 联网检索：
  - ducksearch
- 生成模型：
  - glm-4-air
- 重排序：
  - bge-reranker-large
- 即时压缩：
  - Lingua2
- 接口开发：
  - fastapi
- 文档存储or历史对话存储：
  - mysql
- 评估管道：
  - ragas

## 工作流程：

1. 启动API下的api服务
2. 使用curl -X POST "http://127.0.0.1:8000/ask" -H "Content-Type: application/json" -d "{\"q\": \"输入你的query\"}"，向API发送请求
3. API接收到请求后，异步调用ducksearch、Elasticsearch、milvussearch检索，获取混合检索结果
4. 对于Elasticsearch和milvussearch检索的片段（它们的数据组织形式是一致的）使用hash_id去重，并将来源于相同文档的片段按顺序合并
5. 将上述处理过后的数据与ducksearch搜索结果合并，进入重排序管道，并top-k采样
6. 重排序过后的数据增加meta信息，如来源、时间、url连接、标题等信息
7. 进入即时压缩管道，对每个片段进行异步压缩，并将信息压缩后的结果组织成文本，输入模型
8. 模型流式输出text

## 以下是各个模块的描述：

1. API: 负责接收用户请求，并异步调用各个检索模块，并将结果进行合并，返回给用户。
2. ducksearch: 负责联网检索，它可以从多个网站上爬取数据，并进入文本处理管道，生成json格式的输出，{'content':,'meta':[{},{},{}]}，需要VPN环境。
3. Elasticsearch: 负责全文检索，它可以对文本进行BM25检索，设定阈值和top_k，它的数据格式与milvussearch的格式相同，方便后续统一处理。
4. milvussearch: 负责向量检索，在构建时，考虑文档的连贯性，分别建立了全文(摘要处理)向量检索和片段向量检索，全文向量检索将会限制片段检索的实际返回结果。
5. LLM：用于统一各家厂商LLM的生成管道，输入统一为messages,输出统一为{'role':'assistant','content':''}，目前为了适应流式输出，将zhipuAI的输出改成了文本输出。
6. reranker：负责重排序管道，目前可以使用huggingface更换模型进行调整（需要GPU环境，至少3G显存）。
7. Lingua2: 位于Instantcompression中，使用小模型来提取有效信息，提示词工程+微调+统计算法(需要GPU环境，至少3G显存)。LongLLMLingua未经测试，至少6G(GPU)显存环境，后续考虑。
8. fastapi: 负责API的开发，使用fastapi框架，并使用uvicorn作为服务器，支持http和websocket协议。
9. mysql: 后续负责存储历史对话数据，包括用户输入、系统回复、时间、来源等信息。当前使用mysql存储文本，milvus存储索引的方式实现向量搜索。
10. test-evaluate: 用于评估模型，目前接入RAGAS的部分指标，其他指标待定。

## 一、LLMContextRecall（模型输出召回）：

通过使用带注释的答案和检索到的上下文估计 TP 和 FN 来估计上下文回忆。如果使用LLM，则无需注释，通俗来讲是查看LLM的输出用到了检索到的上下文的哪些句子，这些句子的占比反映了检索到的上下文的文本质量，分数越大，说明上下文的质量越高。

## 二、ContextEntityRecall（上下文实体召回）：

计算检索到的文本和正确答案存在的实体的召回率，查看检索到的文本能否完全覆盖正确答案中的实体。令 CN 为上下文中存在的实体集，GN 为事实中存在的实体集。如果这个数量是 1，我们可以说检索机制已经检索到涵盖基本事实中存在的所有实体。

## 三、LLMContextPrecisionWithReference（模型预测输出顺序和参考的比较）：

模型根据真实答案对上下文的重要性进行重新排序，查看模型的排序是否与原检索的排序一致。越接近1，说明越重要的上下文被优先检索到，检索系统的效果越好。

使用朴素向量搜索+LLM生成框架的测试数据作为数据基线，共使用十个人为设计的问题，测试结果在Test/evaluate/result/RAG_1.csv中。

## 第一期在上述基础上，添加以下可能会改变检索能力的模块：

1. 联网搜索+全文搜索的混合搜索
2. 重排序管道和文本压缩管道

***第一期测试数据在上述文件夹的RAG_2.csv中。***

## 11、Embedding:

用于统一各家厂商的向量模型接口，用于向量检索和向量数据库搭建。

## 12、other:

其他验证过但未加入本期的模块。

## 注意事项：

- api启动后需要等到1-2分钟，等模型加载到GPU中。
- Dataset下的数据库函数根据之前工作复制而来，未经过严格测试，仅供参考。
- 数据的导入和数据库构建schema视具体情况而定，在此做简单描述：

### 数据库来源：法律文件

elasticsearch和mysql的文档存储共用一个数据集组织形式：

- hash: 切片的hash值,用于验证文档完整性和去重
- content: 切片的文本内容
- source: 切片的来源，如法律文件名
- publish_time: 切片的发布时间
- effective_time: 切片的生效时间
- category: 切片的类别，如民法、刑法等
- publish_unit: 切片发布机构

### milvus的存储数据格式：

- FULL_TEXT_HASH: 切片的来源id，例如文件名的hash值，用于分组合并
- SLICE_HASH: 切片的hash，与mysql的文档id对应
- slice_vector: 切片的向量表示，向量维度512

## 目前测试遇到的问题：

1. 各模块测试还不够，没有找到最优的运行顺序和检索到的数据组织方式。
2. ElasticSearch根据关键词搜索，但是严重依赖分词规则，默认规则无法与垂直领域的分词逻辑相符合，导致搜索效果变差。例如会将"一个中国理论"拆分成“一个” “中国” “理论”，“台独”拆分为“台”、“独”。
3. 联网搜索duckgo langchain实现极其不稳定，1是连接在异步调用下会进行重连，如果第一次练级不上，后续的重连可能将会导致远程主机请求阻塞。2由于是vpn连接，使用中文搜索国内的一些网页有可能导致无法打开的问题导致报错 3无法处理文件下载链接。

其不稳定因素使得整个检索过程延长。

4. 将混合检索结果输入到重排序模型，可能会导致问答对的联网搜索效果具有较高的rank，这是因为当前所选取的模型训练对就是问答对，而不是真正意义上的问和答。当采样数量较少时，传入模型的上下文就只有联网搜索的评论、问答等，使得知识库中的文段得不到很好的排和展示。

5. duckgo联网搜索的清洗仍然不到位。

6. 联网搜索的结果具有真假性，质量没有知识库那么高。

7. 增加了重排序和即时压缩管道，响应时间延长了大约4秒。

8. 目前数据集和测试样例都较小，无法真实反映系统的性能。

